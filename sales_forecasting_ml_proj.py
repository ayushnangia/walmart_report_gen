# -*- coding: utf-8 -*-
"""sales-forecasting-ML-Proj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zrxhYNZ2j6McZYVHqXki2m1jeSd4N-LI

# Importing Necessary Libraries and Data
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np      # To use np.arrays
import pandas as pd     # To use dataframes
from pandas.plotting import autocorrelation_plot as auto_corr

# To plot
import matplotlib.pyplot as plt
# %matplotlib inline
import matplotlib as mpl
import seaborn as sns

#For date-time
import math
from datetime import datetime
from datetime import timedelta

# Another imports if needs
import itertools
import statsmodels.api as sm
import statsmodels.tsa.api as smt
import statsmodels.formula.api as smf

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.metrics import accuracy_score, balanced_accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn import metrics
from sklearn import preprocessing




import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

pd.options.display.max_columns=100 # to see coldf.umns

df_store = pd.read_csv('walmart/walmart/stores.csv') #store data

df_train = pd.read_csv('walmart/walmart/train.csv') # train set

df_features = pd.read_csv('walmart/walmart/features.csv') #external information

"""# First Look to Data and Merging Three Dataframes"""

df_store.head()

df_train.head()

df_features.head()

# merging 3 different sets
df = df_train.merge(df_features, on=['Store', 'Date'], how='inner').merge(df_store, on=['Store'], how='inner')
df.head(5)

df.tail(5)

df.drop(['IsHoliday_y'], axis=1,inplace=True) # removing dublicated column

df.rename(columns={'IsHoliday_x':'IsHoliday'},inplace=True) # rename the column

df.head(5) # last ready data set

df.shape

"""# Store & Department Numbers"""

df['Store'].nunique() # number of different values

df['Dept'].nunique() # number of different values

store_dept_table = pd.pivot_table(df, index='Store', columns='Dept',
                                  values='Weekly_Sales', aggfunc=np.mean)
display(store_dept_table)

df.loc[df['Weekly_Sales']<=0]

"""1358 rows in 421570 rows means 0.3%, so I can delete and ignore these rows which contains wrong sales values."""

df = df.loc[df['Weekly_Sales'] > 0]

df.shape # new data shape

df.head()

"""# Date"""

df['Date'].head(5).append(df['Date'].tail(5)) # to see first and last 5 rows.

"""Our data is from 5th of February 2010 to 26th of October 2012.

# IsHoliday column
"""

sns.barplot(x='IsHoliday', y='Weekly_Sales', data=df)

df_holiday = df.loc[df['IsHoliday']==True]
df_holiday['Date'].unique()

df_not_holiday = df.loc[df['IsHoliday']==False]
df_not_holiday['Date'].nunique()

"""All holidays are not in the data. There are 4 holiday values such as;

Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13

Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13

Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13

Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13


After the 07-Sep-2012 holidays are in test set for prediction. When we look at the data, average weekly sales for holidays are significantly higher than not-holiday days. In train data, there are 133 weeks for non-holiday and 10 weeks for holiday.
"""

# Super bowl dates in train set
df.loc[(df['Date'] == '2010-02-12')|(df['Date'] == '2011-02-11')|(df['Date'] == '2012-02-10'),'Super_Bowl'] = True
df.loc[(df['Date'] != '2010-02-12')&(df['Date'] != '2011-02-11')&(df['Date'] != '2012-02-10'),'Super_Bowl'] = False

check=df.loc[df['Date'] == '2010-02-12']
check.head()

# Labor day dates in train set
df.loc[(df['Date'] == '2010-09-10')|(df['Date'] == '2011-09-09')|(df['Date'] == '2012-09-07'),'Labor_Day'] = True
df.loc[(df['Date'] != '2010-09-10')&(df['Date'] != '2011-09-09')&(df['Date'] != '2012-09-07'),'Labor_Day'] = False

# Thanksgiving dates in train set
df.loc[(df['Date'] == '2010-11-26')|(df['Date'] == '2011-11-25'),'Thanksgiving'] = True
df.loc[(df['Date'] != '2010-11-26')&(df['Date'] != '2011-11-25'),'Thanksgiving'] = False

#Christmas dates in train set
df.loc[(df['Date'] == '2010-12-31')|(df['Date'] == '2011-12-30'),'Christmas'] = True
df.loc[(df['Date'] != '2010-12-31')&(df['Date'] != '2011-12-30'),'Christmas'] = False

sns.barplot(x='Christmas', y='Weekly_Sales', data=df) # Christmas holiday vs not-Christmas

sns.barplot(x='Thanksgiving', y='Weekly_Sales', data=df) # Thanksgiving holiday vs not-thanksgiving

sns.barplot(x='Super_Bowl', y='Weekly_Sales', data=df) # Super bowl holiday vs not-super bowl

sns.barplot(x='Labor_Day', y='Weekly_Sales', data=df) # Labor day holiday vs not-labor day

"""# Type Effect on Holidays"""

df.groupby(['Christmas','Type'])['Weekly_Sales'].mean()

df.groupby(['Labor_Day','Type'])['Weekly_Sales'].mean()

df.groupby(['Thanksgiving','Type'])['Weekly_Sales'].mean()

df.groupby(['Super_Bowl','Type'])['Weekly_Sales'].mean()

my_data = [48.88, 37.77 , 13.33 ]  ## already given
my_labels = 'Type A','Type B', 'Type C'
plt.pie(my_data,labels=my_labels,autopct='%1.1f%%', textprops={'fontsize': 15})
plt.axis('equal')
mpl.rcParams.update({'font.size': 20})

plt.show()

df.groupby('IsHoliday')['Weekly_Sales'].mean()

"""Nearly, half of the stores are belongs to Type A."""

plt.style.use('seaborn-poster')
labels = ['Thanksgiving', 'Super_Bowl', 'Labor_Day', 'Christmas']
A_means = [27397.77, 20612.75, 20004.26, 18310.16]
B_means = [18733.97, 12463.41, 12080.75, 11483.97]
C_means = [9696.56,10179.27,9893.45,8031.52]

x = np.arange(len(labels))  # the label locations
width = 0.25  # the width of the bars

fig, ax = plt.subplots(figsize=(16, 8))
rects1 = ax.bar(x - width, A_means, width, label='Type_A')
rects2 = ax.bar(x , B_means, width, label='Type_B')
rects3 = ax.bar(x + width, C_means, width, label='Type_C')

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Weekly Avg Sales')
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend()


def autolabel(rects):
    """Attach a text label above each bar in *rects*, displaying its height."""
    for rect in rects:
        height = rect.get_height()
        ax.annotate('{}'.format(height),
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')


autolabel(rects1)
autolabel(rects2)
autolabel(rects3)

plt.axhline(y=17094.30,color='r') # holidays avg
plt.axhline(y=15952.82,color='green') # not-holiday avg

fig.tight_layout()

plt.show()

df.sort_values(by='Weekly_Sales',ascending=False).head(5)

df_store.groupby('Type').describe()['Size'].round(2)

plt.figure(figsize=(10,8))
fig = sns.boxplot(x='Type', y='Size', data=df, showfliers=False)

df.isna().sum()

df = df.fillna(0) # filling null's with 0

df.isna().sum() # last null check

df.describe() # to see weird statistical things

x = df['Dept']
y = df['Weekly_Sales']
plt.figure(figsize=(15,5))
plt.title('Weekly Sales by Department')
plt.xlabel('Departments')
plt.ylabel('Weekly Sales')
plt.scatter(x,y)
plt.show()

plt.figure(figsize=(30,10))
fig = sns.barplot(x='Dept', y='Weekly_Sales', data=df)

x = df['Store']
y = df['Weekly_Sales']
plt.figure(figsize=(15,5))
plt.title('Weekly Sales by Store')
plt.xlabel('Stores')
plt.ylabel('Weekly Sales')
plt.scatter(x,y)
plt.show()

plt.figure(figsize=(20,6))
fig = sns.barplot(x='Store', y='Weekly_Sales', data=df)

df["Date"] = pd.to_datetime(df["Date"]) # convert to datetime
df['week'] =df['Date'].dt.week
df['month'] =df['Date'].dt.month
df['year'] =df['Date'].dt.year

df.groupby('month')['Weekly_Sales'].mean() # to see the best months for sales

df.groupby('year')['Weekly_Sales'].mean() # to see the best years for sales

monthly_sales = pd.pivot_table(df, values = "Weekly_Sales", columns = "year", index = "month")
monthly_sales.plot()

fig = sns.barplot(x='month', y='Weekly_Sales', data=df)

df.groupby('week')['Weekly_Sales'].mean().sort_values(ascending=False).head()

weekly_sales = pd.pivot_table(df, values = "Weekly_Sales", columns = "year", index = "week")
weekly_sales.plot()

plt.figure(figsize=(20,6))
fig = sns.barplot(x='week', y='Weekly_Sales', data=df)

"""# Fuel Price, CPI , Unemployment , Temperature Effects"""

fuel_price = pd.pivot_table(df, values = "Weekly_Sales", index= "Fuel_Price")
fuel_price.plot()

temp = pd.pivot_table(df, values = "Weekly_Sales", index= "Temperature")
temp.plot()

CPI = pd.pivot_table(df, values = "Weekly_Sales", index= "CPI")
CPI.plot()

unemployment = pd.pivot_table(df, values = "Weekly_Sales", index= "Unemployment")
unemployment.plot()

df.to_csv('clean_data.csv') # assign new data frame to csv for using after here

df = pd.read_csv('./clean_data.csv')

df.drop(columns=['Unnamed: 0'],inplace=True)

df['Date'] = pd.to_datetime(df['Date']) # changing datetime to divide if needs

"""# Encoding the Data

For preprocessing our data, I will change holidays boolean values to 0-1 and replace type of the stores from A, B, C to 1, 2, 3.
"""

df_encoded = df.copy() # to keep original dataframe taking copy of it

type_group = {'A':1, 'B': 2, 'C': 3}  # changing A,B,C to 1-2-3
df_encoded['Type'] = df_encoded['Type'].replace(type_group)

df_encoded['Super_Bowl'] = df_encoded['Super_Bowl'].astype(bool).astype(int) # changing T,F to 0-1

df_encoded['Thanksgiving'] = df_encoded['Thanksgiving'].astype(bool).astype(int) # changing T,F to 0-1

df_encoded['Labor_Day'] = df_encoded['Labor_Day'].astype(bool).astype(int) # changing T,F to 0-1

df_encoded['Christmas'] = df_encoded['Christmas'].astype(bool).astype(int) # changing T,F to 0-1

df_encoded['IsHoliday'] = df_encoded['IsHoliday'].astype(bool).astype(int) # changing T,F to 0-1

df_new = df_encoded.copy() # taking the copy of encoded df to keep it original

"""# Observation of Interactions between Features

Firstly, i will drop divided holiday columns from my data and try without them. To keep my encoded data safe, I assigned my dataframe to new one and I will use for this.
"""

drop_col = ['Super_Bowl','Labor_Day','Thanksgiving','Christmas']
df_new.drop(drop_col, axis=1, inplace=True) # dropping columns

plt.figure(figsize = (12,10))
sns.heatmap(df_new.corr().abs())    # To see the correlations
plt.show()

"""Temperature, unemployment, CPI have no significant effect on weekly sales, so I will drop them. Also, Markdown 4 and 5 highly correlated with Markdown 1. So, I will drop them also. It can create multicollinearity problem, maybe. So, first I will try without them."""

drop_col = ['Temperature','MarkDown4','MarkDown5','CPI','Unemployment']
df_new.drop(drop_col, axis=1, inplace=True) # dropping columns

plt.figure(figsize = (12,10))
sns.heatmap(df_new.corr().abs())    # To see the correlations without dropping columns
plt.show()

"""Size and type are highly correlated with weekly sales. Also, department and store are correlated with sales."""

df_new = df_new.sort_values(by='Date', ascending=True) # sorting according to date

#df_new_copy = df.resample('W').mean()
#plt.figure(figsize=(16,6))
#df_new_copy['Weekly_Sales'].plot()
#plt.title('Average Sales - Weekly')
#plt.show()

"""# Creating Train-Test Splits

Our date column has continuos values, to keep the date features continue, I will not take random splitting. so, I split data manually according to 70%.
"""

train_data = df_new[:int(0.7*(len(df_new)))] # taking train part
test_data = df_new[int(0.7*(len(df_new))):] # taking test part

target = "Weekly_Sales"
used_cols = [c for c in df_new.columns.to_list() if c not in [target]] # all columns except weekly sales

X_train = train_data[used_cols]
X_test = test_data[used_cols]
y_train = train_data[target]
y_test = test_data[target]

X = df_new[used_cols] # to keep train and test X values together

"""We have enough information in our date such as week of the year. So, I drop date columns."""

X_train = X_train.drop(['Date'], axis=1) # dropping date from train
X_test = X_test.drop(['Date'], axis=1) # dropping date from test

"""# Metric Definition Function

Our metric is not calculated as default from ready models. It is weighed error so, I will use function below to calculate it.
"""

def wmae_test(test, pred): # WMAE for test
    weights = X_test['IsHoliday'].apply(lambda is_holiday:5 if is_holiday else 1)
    error = np.sum(weights * np.abs(test - pred), axis=0) / np.sum(weights)
    return error

"""# Random Forest Regressor

To tune the regressor, I can use gridsearch but it takes too much time for this type of data which has many rows and columns. So, I choose regressor parameters manually. I changed the parameters each time and try to find the best result.
"""

rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1, max_depth=35,
                           max_features = 'sqrt',min_samples_split = 10)

from sklearn.preprocessing import RobustScaler
scaler = RobustScaler()



#making pipe tp use scaler and regressor together
pipe = make_pipeline(scaler,rf)

pipe.fit(X_train, y_train)

# predictions on train set
y_pred = pipe.predict(X_train)

# predictions on test set
y_pred_test = pipe.predict(X_test)

# test_data_copy = test_data.copy()
# test_data_copy['Weekly_Sales'] = y_pred_test
# test_data_copy.resample['Store']
# print(test_data_copy)

wmae_test(y_test, y_pred_test)

"""For the first trial, my weighted error is around 5850.

# To See Feature Importance
"""

X = X.drop(['Date'], axis=1) #dropping date column from X

importances = rf.feature_importances_
feature_importances = pd.DataFrame(rf.feature_importances_, index = X_train.columns, columns=['Importance'])
print(feature_importances)
std = np.std([tree.feature_importances_ for tree in rf.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

# Printing the feature ranking
print("Feature ranking:")

for f in range(X.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plotting the feature importances of the forest
plt.figure(figsize=(25,10))
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices],
       color="g", yerr=std[indices], align="center")
plt.xticks(range(X.shape[1]), X_train.columns[indices])
plt.xlim([-1, X.shape[1]])
plt.show()

"""After looking feature importance, I dropped least important 3-4 features and tried the model. I found the best result when I dropped month column which is highly correlated with week."""

X1_train = X_train.drop(['month'], axis=1) # dropping month
X1_test = X_test.drop(['month'], axis=1)

print(X1_train.describe)

"""# Model Again without Month"""

rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1, max_depth=35,
                           max_features = 'sqrt',min_samples_split = 10)

scaler=RobustScaler()
pipe = make_pipeline(scaler,rf)

pipe.fit(X1_train, y_train)

# predictions on train set
y_pred = pipe.predict(X1_train)

# predictions on test set
y_pred_test = pipe.predict(X1_test)

print(len(y_pred_test))
print(len(y_test))



wmae_test(y_test, y_pred_test)

"""It gives better results than baseline.

# Model with Whole Data

Now, I want to make sure that my model will learn from the columns which I dropped or not. So, I will apply my model to whole encoded data again.
"""

# splitting train-test to whole dataset
train_data_enc = df_encoded[:int(0.7*(len(df_encoded)))]
test_data_enc = df_encoded[int(0.7*(len(df_encoded))):]

print(df_encoded.columns)

target = "Weekly_Sales"
used_cols1 = [c for c in df_encoded.columns.to_list() if c not in [target]] # all columns except price

X_train_enc = train_data_enc[used_cols1]
X_test_enc = test_data_enc[used_cols1]
y_train_enc = train_data_enc[target]
y_test_enc = test_data_enc[target]

X_enc = df_encoded[used_cols1] # to get together train,test splits

X_enc = X_enc.drop(['Date'], axis=1) #dropping date column for whole X

X_train_enc = X_train_enc.drop(['Date'], axis=1) # dropping date from train and test
X_test_enc= X_test_enc.drop(['Date'], axis=1)

rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1, max_depth=35,
                           max_features = 'sqrt',min_samples_split = 10)

scaler=RobustScaler()
pipe = make_pipeline(scaler,rf)

pipe.fit(X_train_enc, y_train_enc)

# predictions on train set
y_pred_enc = pipe.predict(X_train_enc)

# predictions on test set
y_pred_test_enc = pipe.predict(X_test_enc)

wmae_test(y_test_enc, y_pred_test_enc)

"""We found better results for whole data, it means our model can learn from columns which I dropped before.

# Feature Importance for Whole Encoded Dataset
"""

importances = rf.feature_importances_
feature_importances_2 = pd.DataFrame(rf.feature_importances_, index = X_enc.columns, columns=['Importance'])
print(feature_importances_2)
print(len(feature_importances_2))
std = np.std([tree.feature_importances_ for tree in rf.estimators_],
             axis=0)
indices = np.argsort(importances)[::-1]

# Printing the feature ranking
print("Feature ranking:")

for f in range(X_enc.shape[1]):
    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))

# Plotting the feature importances of the forest
plt.figure(figsize=(40,20))
plt.title("Feature importances")
plt.bar(range(X_enc.shape[1]), importances[indices],
       color="g", yerr=std[indices], align="center")
plt.xticks(range(X_enc.shape[1]), X_enc.columns[indices])
plt.xlim([-1, X_enc.shape[1]])
plt.show()

"""According to feature importance, I dropped some columns from whole set and try my model again."""

df_encoded_new = df_encoded.copy() # taking copy of encoded data to keep it without change.
df_encoded_new.drop(drop_col, axis=1, inplace=True)

"""# Model According to Feature Importance"""

#train-test splitting
train_data_enc_new = df_encoded_new[:int(0.7*(len(df_encoded_new)))]
test_data_enc_new = df_encoded_new[int(0.7*(len(df_encoded_new))):]

target = "Weekly_Sales"
used_cols2 = [c for c in df_encoded_new.columns.to_list() if c not in [target]] # all columns except price

X_train_enc1 = train_data_enc_new[used_cols2]
X_test_enc1 = test_data_enc_new[used_cols2]
y_train_enc1 = train_data_enc_new[target]
y_test_enc1 = test_data_enc_new[target]

#droping date from train-test
X_train_enc1 = X_train_enc1.drop(['Date'], axis=1)
X_test_enc1= X_test_enc1.drop(['Date'], axis=1)

print(X_train_enc1.columns)

rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1, max_depth=40,
                           max_features = 'log2',min_samples_split = 10)

scaler=RobustScaler()
pipe = make_pipeline(scaler,rf)

pipe.fit(X_train_enc1, y_train_enc1)

# predictions on train set
y_pred_enc = pipe.predict(X_train_enc1)

# predictions on test set
y_pred_test_enc = pipe.predict(X_test_enc1)

y_pred_enc_best = y_pred_enc
y_pred_test_enc_best = y_pred_test_enc

pipe.score(X_test_enc1,y_test_enc1)

wmae_test(y_test_enc1, y_pred_test_enc)

"""I found best results with doing feature selection from whole encoded dataset.

# Model with Dropping Month Column

With the same dateset before, I try to model again without month column.
"""

df_encoded_new1 = df_encoded.copy()
df_encoded_new1.drop(drop_col, axis=1, inplace=True)

df_encoded_new1 = df_encoded_new1.drop(['Date'], axis=1)

df_encoded_new1 = df_encoded_new1.drop(['month'], axis=1)

#train-test split
train_data_enc_new1 = df_encoded_new1[:int(0.7*(len(df_encoded_new1)))]
test_data_enc_new1 = df_encoded_new1[int(0.7*(len(df_encoded_new1))):]

target = "Weekly_Sales"
used_cols3 = [c for c in df_encoded_new1.columns.to_list() if c not in [target]] # all columns except price

X_train_enc2 = train_data_enc_new1[used_cols3]
X_test_enc2 = test_data_enc_new1[used_cols3]
y_train_enc2 = train_data_enc_new1[target]
y_test_enc2 = test_data_enc_new1[target]

#modeling part
pipe = make_pipeline(scaler,rf)

pipe.fit(X_train_enc2, y_train_enc2)

# predictions on train set
y_pred_enc = pipe.predict(X_train_enc2)

# predictions on test set
y_pred_test_enc = pipe.predict(X_test_enc2)

pipe.score(X_test_enc2,y_test_enc2)

wmae_test(y_test_enc2, y_pred_test_enc)

df_results = pd.DataFrame(columns=["Model", "Info",'WMAE']) # result df for showing results together

df_results = df_results.append({
     "Model": 'RandomForestRegressor' ,
      "Info": 'w/out divided holiday columns' ,
       'WMAE' : 5850}, ignore_index=True)

df_results = df_results.append({
     "Model": 'RandomForestRegressor' ,
      "Info": 'w/out month column' ,
       'WMAE' : 5494}, ignore_index=True)
df_results = df_results.append({
     "Model": 'RandomForestRegressor' ,
      "Info": 'whole data' ,
       'WMAE' : 2450}, ignore_index=True)
df_results = df_results.append({
     "Model": 'RandomForestRegressor' ,
      "Info": 'whole data with feature selection' ,
       'WMAE' : 1801}, ignore_index=True)
df_results = df_results.append({
     "Model": 'RandomForestRegressor' ,
      "Info": 'whole data with feature selection w/out month' ,
       'WMAE' : 2093}, ignore_index=True)

df_results

"""The best results belongs to whole data set with feature selection."""

